# spark_jobs.py
import json
import boto3
import logging
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp

# -------------------------------
# 1Ô∏è‚É£ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MinIO –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
# -------------------------------
# –í Docker –æ–∫—Ä—É–∂–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Å–µ—Ä–≤–∏—Å–∞ 'minio'
minio_endpoint = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
access_key = os.getenv("MINIO_ROOT_USER")
secret_key = os.getenv("MINIO_ROOT_PASSWORD")
bucket_name = os.getenv("MINIO_BUCKET", "dev")
object_key = "data/temperature.json"

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
if not access_key or not secret_key:
    raise ValueError("‚ùå MinIO credentials not found in environment variables. Set MINIO_ROOT_USER and MINIO_ROOT_PASSWORD.")

logging.info(f"üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ MinIO: {minio_endpoint}")

s3 = boto3.client(
    's3',
    endpoint_url=minio_endpoint,
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key
)

# -------------------------------
# 2Ô∏è‚É£ –ü–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–∫—Ç –∏–∑ MinIO
# -------------------------------
obj = s3.get_object(Bucket=bucket_name, Key=object_key)
data = json.loads(obj['Body'].read())

# -------------------------------
# 3Ô∏è‚É£ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PySpark —Å –¥—Ä–∞–π–≤–µ—Ä–æ–º PostgreSQL
# -------------------------------
jdbc_driver_path = os.getenv("JDBC_DRIVER_PATH", "/opt/libs/postgresql-42.7.7.jar")
spark = SparkSession.builder \
    .appName("MinIO_to_Postgres") \
    .config("spark.jars", jdbc_driver_path) \
    .getOrCreate()

# -------------------------------
# 4Ô∏è‚É£ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ JSON –≤ DataFrame
# -------------------------------
stations_list = [
    (s["id"], s["device_id"], s["name"], s["location"]["latitude"], s["location"]["longitude"])
    for s in data["metadata"]["stations"]
]
stations_df = spark.createDataFrame(stations_list, ["station_id", "device_id", "name", "latitude", "longitude"])

readings_list = []
for item in data["items"]:
    ts = item["timestamp"]
    for r in item["readings"]:
        readings_list.append((r["station_id"], ts, r["value"]))

readings_df = spark.createDataFrame(readings_list, ["station_id", "timestamp", "value"])

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ TimestampType –¥–ª—è PostgreSQL
readings_df = readings_df.withColumn(
    "timestamp",
    to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ssXXX")
)

# -------------------------------
# 5Ô∏è‚É£ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
# -------------------------------
# –í Docker –æ–∫—Ä—É–∂–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Å–µ—Ä–≤–∏—Å–∞ 'postgres', –∞ –Ω–µ localhost
postgres_host = os.getenv("POSTGRES_HOST", "postgres")
postgres_port = os.getenv("POSTGRES_INTERNAL_PORT", "5432")  # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –ø–æ—Ä—Ç –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
postgres_db = os.getenv("POSTGRES_DB", "airflow")
postgres_user = os.getenv("POSTGRES_USER")
postgres_password = os.getenv("POSTGRES_PASSWORD")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
if not postgres_user or not postgres_password:
    raise ValueError("‚ùå PostgreSQL credentials not found in environment variables. Set POSTGRES_USER and POSTGRES_PASSWORD.")

jdbc_url = f"jdbc:postgresql://{postgres_host}:{postgres_port}/{postgres_db}"
logging.info(f"üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL: {postgres_host}:{postgres_port}/{postgres_db}")
jdbc_props = {
    "user": postgres_user,
    "password": postgres_password,
    "driver": "org.postgresql.Driver"
}

# -------------------------------
# 6Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL
# -------------------------------
stations_df.write.jdbc(jdbc_url, "stations", mode="ignore", properties=jdbc_props)
readings_df.write.jdbc(jdbc_url, "readings", mode="append", properties=jdbc_props)

logging.info("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ PostgreSQL")

# -------------------------------
# 7Ô∏è‚É£ –ó–∞–∫—Ä—ã—Ç–∏–µ Spark
# -------------------------------
spark.stop()