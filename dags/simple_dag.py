from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.models import TaskInstance
from datetime import timedelta
import logging
import json
import requests
import os
from io import BytesIO

from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from botocore.exceptions import BotoCoreError, ClientError

# Ğ˜Ğ¼Ñ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² Airflow Admin > Connections
AIRFLOW_CONN_ID = 'minio_default'

# ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
BUCKET_NAME = os.getenv('MINIO_BUCKET', 'dev')
OBJECT_KEY = 'data/temperature.json'
API_URL = os.getenv('TEMPERATURE_API_URL', 'https://api.data.gov.sg/v1/environment/air-temperature')

default_args = {
    'owner': 'airflow',
    'retries': 2,
    'retry_delay': timedelta(minutes=1),
}


def fetch_api_data(ti: TaskInstance) -> None:
    logging.info(f"ğŸ” Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ {API_URL}")

    try:
        response = requests.get(API_URL)
        response.raise_for_status()
        data = response.json()
        logging.info("âœ… Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹")
        ti.xcom_push(key='api_data', value=data)
    except requests.RequestException as e:
        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")
        raise


def upload_to_minio(ti: TaskInstance) -> None:
    logging.info("ğŸ“¦ ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· XCom")
    data = ti.xcom_pull(task_ids='fetch_data_task', key='api_data')

    if data is None:
        logging.error("âŒ ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² XCom â€” Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ")
        raise ValueError("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸")

    content = json.dumps(data).encode("utf-8")

    try:
        logging.info("ğŸ”— ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğº MinIO Ñ‡ĞµÑ€ĞµĞ· Airflow S3Hook")
        s3_hook = S3Hook(aws_conn_id=AIRFLOW_CONN_ID)

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ±Ğ°ĞºĞµÑ‚Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
        existing_buckets = [bucket['Name'] for bucket in s3_hook.get_conn().list_buckets().get('Buckets', [])]
        if BUCKET_NAME not in existing_buckets:
            logging.info(f"ğŸª£ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ±Ğ°ĞºĞµÑ‚: {BUCKET_NAME}")
            s3_hook.get_conn().create_bucket(Bucket=BUCKET_NAME)

        logging.info(f"â¬†ï¸ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚: {OBJECT_KEY}")
        s3_hook.load_bytes(
            bytes_data=content,
            key=OBJECT_KEY,
            bucket_name=BUCKET_NAME,
            replace=True,
        )
        logging.info("âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ² MinIO ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°")

    except (BotoCoreError, ClientError) as e:
        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ MinIO: {e}")
        raise


with DAG(
    dag_id='api_to_minio_with_conn',
    default_args=default_args,
    start_date=days_ago(1),
    schedule_interval='@daily',
    catchup=False,
    description='DAG Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· API Ğ² MinIO Ñ‡ĞµÑ€ĞµĞ· Airflow Connection',
    tags=['minio', 'api'],
) as dag:

    fetch_task = PythonOperator(
        task_id='fetch_data_task',
        python_callable=fetch_api_data,
    )

    upload_task = PythonOperator(
        task_id='upload_to_minio',
        python_callable=upload_to_minio,
    )

    fetch_task >> upload_task