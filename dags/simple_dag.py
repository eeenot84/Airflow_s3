from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.models import TaskInstance
from datetime import timedelta
import logging
import json
import requests
import os
import subprocess
from io import BytesIO

from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from botocore.exceptions import BotoCoreError, ClientError

# Ğ˜Ğ¼Ñ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² Airflow Admin > Connections
AIRFLOW_CONN_ID = 'minio_default'

# ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
BUCKET_NAME = os.getenv('MINIO_BUCKET', 'dev')
OBJECT_KEY = 'data/temperature.json'
API_URL = os.getenv('TEMPERATURE_API_URL', 'https://api.data.gov.sg/v1/environment/air-temperature')

default_args = {
    'owner': 'airflow',
    'retries': 2,
    'retry_delay': timedelta(minutes=1),
}


def fetch_api_data(ti: TaskInstance) -> None:
    logging.info(f"ğŸ” Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ {API_URL}")

    try:
        response = requests.get(API_URL)
        response.raise_for_status()
        data = response.json()
        logging.info("âœ… Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹")
        ti.xcom_push(key='api_data', value=data)
    except requests.RequestException as e:
        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")
        raise


def upload_to_minio(ti: TaskInstance) -> None:
    logging.info("ğŸ“¦ ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· XCom")
    data = ti.xcom_pull(task_ids='fetch_data_task', key='api_data')

    if data is None:
        logging.error("âŒ ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² XCom â€” Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ")
        raise ValueError("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸")

    content = json.dumps(data).encode("utf-8")

    try:
        logging.info("ğŸ”— ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğº MinIO Ñ‡ĞµÑ€ĞµĞ· Airflow S3Hook")
        s3_hook = S3Hook(aws_conn_id=AIRFLOW_CONN_ID)

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ±Ğ°ĞºĞµÑ‚Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
        existing_buckets = [bucket['Name'] for bucket in s3_hook.get_conn().list_buckets().get('Buckets', [])]
        if BUCKET_NAME not in existing_buckets:
            logging.info(f"ğŸª£ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ±Ğ°ĞºĞµÑ‚: {BUCKET_NAME}")
            s3_hook.get_conn().create_bucket(Bucket=BUCKET_NAME)

        logging.info(f"â¬†ï¸ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚: {OBJECT_KEY}")
        s3_hook.load_bytes(
            bytes_data=content,
            key=OBJECT_KEY,
            bucket_name=BUCKET_NAME,
            replace=True,
        )
        logging.info("âœ… Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ² MinIO ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°")

    except (BotoCoreError, ClientError) as e:
        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ MinIO: {e}")
        raise


def run_spark_job(ti: TaskInstance) -> None:
    """Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Spark job Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· MinIO Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ² PostgreSQL"""
    spark_script_path = "/opt/airflow/scripts/spark_jobs.py"
    
    logging.info(f"ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ° PySpark: {spark_script_path}")
    
    try:
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°
        if not os.path.exists(spark_script_path):
            raise FileNotFoundError(f"âŒ Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {spark_script_path}")
        
        # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Spark job
        result = subprocess.run(
            ["python3", spark_script_path],
            capture_output=True,
            text=True,
            check=True,
            env=dict(os.environ)  # ĞŸĞµÑ€ĞµĞ´Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
        )
        
        logging.info("âœ… PySpark job ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½")
        logging.info(f"ğŸ“„ Ğ’Ñ‹Ğ²Ğ¾Ğ´: {result.stdout}")
        
    except subprocess.CalledProcessError as e:
        logging.error(f"âŒ PySpark Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»ÑÑ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¾Ğ¹:")
        logging.error(f"ğŸ“„ stdout: {e.stdout}")
        logging.error(f"ğŸ“„ stderr: {e.stderr}")
        raise
    except Exception as e:
        logging.error(f"âŒ ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞµ PySpark: {e}")
        raise


with DAG(
    dag_id='api_to_minio_and_postgres',
    default_args=default_args,
    start_date=days_ago(1),
    schedule_interval='@daily',
    catchup=False,
    description='DAG Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· API Ğ² MinIO Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Spark Ğ² PostgreSQL',
    tags=['minio', 'api', 'spark', 'postgres'],
) as dag:

    fetch_task = PythonOperator(
        task_id='fetch_data_task',
        python_callable=fetch_api_data,
    )

    upload_task = PythonOperator(
        task_id='upload_to_minio',
        python_callable=upload_to_minio,
    )

    spark_task = PythonOperator(
        task_id='run_spark_job',
        python_callable=run_spark_job,
    )

    fetch_task >> upload_task >> spark_task